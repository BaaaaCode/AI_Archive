import streamlit as st
import os
from dotenv import load_dotenv

# Fix for UnicodeEncodeError on Windows with Gemini/gRPC
os.environ["PYTHONIOENCODING"] = "utf-8"
os.environ["LANG"] = "C.UTF-8"

# Load environment variables
load_dotenv()

from kiwipiepy import Kiwi
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from pypdf import PdfReader

@st.cache_resource
def get_kiwi():
    return Kiwi()

def tokenize_kiwi(text):
    """
    Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸°ë¡œ í…ìŠ¤íŠ¸ë¥¼ ê³µë°±ìœ¼ë¡œ ì—°ê²°ëœ ë¬¸ìì—´ë¡œ ë³€í™˜
    """
    kiwi = get_kiwi()
    results = kiwi.analyze(text)
    tokens = []
    for result in results:
        for token in result[0]:
            tokens.append(token.form)
    return ' '.join(tokens)

def preprocess_and_chunk(text):
    """
    1. Kiwi í˜•íƒœì†Œ ë¶„ì„
    2. RecursiveCharacterTextSplitterë¡œ ì²­í‚¹ (size=300, overlap=50)
    """
    processed_text = tokenize_kiwi(text)

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=300,
        chunk_overlap=50,
        separators=["\n\n", "\n", " ", ""]
    )
    return text_splitter.split_text(processed_text)

@st.cache_resource
def get_vectorstore(api_key):
    """
    Load existing ChromaDB if available.
    """
    persist_dir = "./antigravity_db"
    
    if os.path.exists(persist_dir) and os.listdir(persist_dir):
        embeddings = GoogleGenerativeAIEmbeddings(
            model="models/embedding-001", 
            google_api_key=api_key
        )
        vectorstore = Chroma(
            persist_directory=persist_dir,
            embedding_function=embeddings,
            collection_name="antigravity_docs"
        )
        return vectorstore
    return None

import chromadb

def build_vectorstore(api_key):
    """
    Build new ChromaDB from data folder.
    Refreshes the DB by deleting the collection via client (safer on Windows).
    """
    persist_dir = "./antigravity_db"
    current_dir = os.path.dirname(os.path.abspath(__file__))
    data_dir = os.path.join(current_dir, "data")
    
    if not os.path.exists(data_dir):
        st.error("Data directory not found.")
        return None

    # Gather all texts
    all_chunks = []
    files = os.listdir(data_dir)
    status_text = st.empty()
    
    progress_bar = st.progress(0)
    for i, file in enumerate(files):
        status_text.text(f"Processing {file}...")
        file_path = os.path.join(data_dir, file)
        
        content = ""
        try:
            if file.lower().endswith(".pdf"):
                pdf = PdfReader(file_path)
                for page in pdf.pages:
                    text = page.extract_text()
                    if text:
                        content += text + "\n"
            else: # Default to text file
                with open(file_path, "r", encoding="utf-8") as f:
                    content = f.read()
            
            if content:
                chunks = preprocess_and_chunk(content)
                all_chunks.extend(chunks)
                
        except Exception as e:
            st.error(f"Error reading {file}: {e}")
            continue
            
        progress_bar.progress((i + 1) / len(files))
    
    status_text.text(f"Generating Embeddings for {len(all_chunks)} chunks...")
    
    embeddings = GoogleGenerativeAIEmbeddings(
        model="models/embedding-001", 
        google_api_key=api_key
    )
    
    # Use PersistentClient to handle collection reset
    client = chromadb.PersistentClient(path=persist_dir)
    
    # Try to delete existing collection to avoid duplicates
    try:
        client.delete_collection("antigravity_docs")
    except ValueError:
        pass # Collection might not exist yet
    
    vectorstore = Chroma.from_texts(
        texts=all_chunks,
        embedding=embeddings,
        collection_name="antigravity_docs",
        client=client # Pass client directly
    )
    status_text.success("DB successfully built!")
    return vectorstore

def ask_gemini(vectorstore, question, api_key, chat_history):
    # 1. Morphological Analysis of the Question (Using Kiwi as replacement for Okt)
    processed_question = tokenize_kiwi(question)
    
    # 2. Retrieve Top 5 Documents (Increased from 3 to improve recall)
    # Return (doc, score) tuples
    docs_with_scores = vectorstore.similarity_search_with_score(processed_question, k=5)
    
    # Extract docs just for context building
    docs = [doc for doc, score in docs_with_scores]
    context = "\n\n".join([doc.page_content for doc in docs])
    
    # Format chat history for context
    # Use last 3 turns to keep prompt size manageable
    recent_history = chat_history[-6:] if len(chat_history) > 6 else chat_history
    formatted_history = ""
    for msg in recent_history:
        role = "User" if msg["role"] == "user" else "Assistant"
        formatted_history += f"{role}: {msg['content']}\n"

    # 3. System Prompt & Generation
    system_prompt = f"""
    ë„ˆëŠ” ë²•ë¥  ì „ë¬¸ê°€ì•¼. ì•„ë˜ì˜ [Context]ì™€ [Chat History]ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ ë‹µë³€í•´ì¤˜.
    ë§Œì•½ [Context]ì— ì—†ëŠ” ë‚´ìš©ì´ë¼ë©´ "ì œê³µëœ ë¬¸ì„œì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µí•´ì¤˜.
    ì˜¤ì§ ì œê³µëœ ë§¥ë½ ì •ë³´ë§Œ ì°¸ê³ í•´ì„œ ë‹µë³€í•´ì•¼ í•´.
    
    [Context]
    {context}
    
    [Chat History]
    {formatted_history}
    
    [Question]
    {question}
    """
    
    # Using gemini-2.5-flash-lite as suggested by user
    llm = ChatGoogleGenerativeAI(
        model="models/gemini-2.5-flash-lite",
        google_api_key=api_key,
        temperature=0
    )
    
    # Create a generator for streaming
    def stream_func():
        for chunk in llm.stream(system_prompt):
            yield chunk.content

    return stream_func(), docs_with_scores

def summarize_references(docs, api_key):
    """
    References are in tokenized format (e.g. 'ì œ 4 ì¡° ...').
    Use AI to reconstruct natural Korean and summarize.
    """
    content = "\n\n".join([doc.page_content for doc in docs])
    
    system_prompt = f"""
    ì•„ë˜ í…ìŠ¤íŠ¸ëŠ” í˜•íƒœì†Œ ë¶„ì„ê¸°ì— ì˜í•´ í† í°í™”ë˜ì–´ ë„ì–´ì“°ê¸°ê°€ ì–´ìƒ‰í•œ í•œêµ­ì–´ ë¬¸ì„œë“¤ì…ë‹ˆë‹¤.
    ì´ ë‚´ìš©ì„ ì½ê³ , ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ë¬¸ì¥ìœ¼ë¡œ ë‹¤ë“¬ì–´ì„œ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.
    ë²•ë¥  ì „ë¬¸ê°€ì²˜ëŸ¼ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì •ë¦¬í•´ì£¼ì„¸ìš”.
    
    [Raw Refereces]
    {content}
    """
    
    llm = ChatGoogleGenerativeAI(
        model="models/gemini-2.5-flash-lite",
        google_api_key=api_key,
        temperature=0
    )
    
    response = llm.invoke(system_prompt)
    return response.content

# 1. Page Config
st.set_page_config(
    page_title="ì‹¤í—˜ìš© ì±—ë´‡",
    page_icon="ğŸ¤–",
    layout="wide"
)

def apply_custom_styles():
    st.markdown("""
    <style>
        /* Import Pretendard Font */
        @import url('https://cdn.jsdelivr.net/gh/orioncactus/pretendard/dist/web/static/pretendard.css');
        
        html, body, [class*="css"] {
            font-family: 'Pretendard', sans-serif;
        }
        
        /* Sidebar Styling */
        [data-testid="stSidebar"] {
            background-color: #f8f9fa;
            border-right: 1px solid #e9ecef;
        }
        
        /* Header Styling */
        h1 {
            background: linear-gradient(to right, #1e3c72, #2a5298);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            font-weight: 800 !important;
        }
        
        h2, h3 {
            color: #2c3e50;
            font-weight: 700 !important;
        }
        
        /* Button Styling */
        .stButton > button {
            background: linear-gradient(45deg, #2a5298, #1e3c72);
            color: white !important;
            border: none;
            border-radius: 10px;
            padding: 0.5rem 1rem;
            font-weight: 600;
            transition: all 0.3s ease;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }
        
        .stButton > button:hover {
            transform: translateY(-2px);
            box-shadow: 0 6px 8px rgba(0,0,0,0.15);
            opacity: 0.9;
        }
        
        /* Chat Input Styling */
        .stChatInput {
            border-radius: 15px !important;
        }
        
        /* Message Styling (Optional tweaks) */
        [data-testid="stChatMessage"] {
            padding: 1rem;
            border-radius: 15px;
            margin-bottom: 1rem;
            box-shadow: 0 2px 4px rgba(0,0,0,0.05);
        }
        
        /* Expander Styling */
        .streamlit-expanderHeader {
            font-weight: 600;
            color: #1e3c72;
        }
    </style>
    """, unsafe_allow_html=True)

apply_custom_styles()

# --- Navigation & Page Management ---

def page_chat(api_key, vectorstore):
    st.title("âš–ï¸ AI Chat")
    st.caption("ğŸš€ RAG ê¸°ë°˜ ë²•ë¥  ìƒë‹´ ì±—ë´‡")

    if not api_key:
        st.error("âš ï¸ API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. [ê´€ë¦¬ì í˜ì´ì§€]ì—ì„œ í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        return
    
    if not vectorstore:
        st.error("âš ï¸ í•™ìŠµëœ ë°ì´í„°ë² ì´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. [ê´€ë¦¬ì í˜ì´ì§€]ì—ì„œ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  DBë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.")
        return

    # Chat Interface
    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("ë²•ë¥  ê´€ë ¨ ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”..."):
        with st.chat_message("user"):
            st.markdown(prompt)
        st.session_state.messages.append({"role": "user", "content": prompt})

        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            with st.spinner("íŒë¡€ì™€ ë²•ë ¹ì„ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
                try:
                    # Note: ask_gemini returns (stream, docs_with_scores)
                    stream, docs_with_scores = ask_gemini(vectorstore, prompt, api_key, st.session_state.messages)
                    
                    # Streaming response
                    response_text = message_placeholder.write_stream(stream)
                    
                    # Reference Section
                    with st.expander("ğŸ“š ì°¸ì¡° ë¬¸ì„œ (AI ìš”ì•½)"):
                         if "ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in response_text and len(response_text) < 150:
                             st.info("ğŸ’¡ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ìš”ì•½ì„ ìƒëµí•©ë‹ˆë‹¤. ì›ë¬¸ ë°ì´í„°ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
                             for i, (doc, score) in enumerate(docs_with_scores):
                                st.caption(f"**Ref {i+1}** (ìœ ì‚¬ë„: {score:.4f})")
                                st.text(doc.page_content)
                         else:
                             # Extract just docs for summary
                             docs = [doc for doc, score in docs_with_scores]
                             with st.spinner("ì°¸ì¡° ë¬¸ì„œ ìš”ì•½ ì¤‘..."):
                                summary = summarize_references(docs, api_key)
                                st.markdown(summary)
                                st.caption("---")
                                for i, (doc, score) in enumerate(docs_with_scores):
                                    st.text(f"[Ref {i+1}] (ê±°ë¦¬: {score:.4f}) {doc.page_content[:100]}...")
                    
                    response = response_text
                except Exception as e:
                    response = f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}"
                    message_placeholder.error(response)
        
        st.session_state.messages.append({"role": "assistant", "content": response})


def page_admin(api_key, current_dir, data_dir):
    st.title("ğŸ› ï¸ ê´€ë¦¬ì ì„¤ì •")
    
    tab1, tab2 = st.tabs(["ğŸ” API ë° DB ì„¤ì •", "ğŸ“‚ ë¬¸ì„œ ë°ì´í„° ê´€ë¦¬"])
    
    with tab1:
        st.subheader("Google API Key ì„¤ì •")
        current_key = api_key if api_key else ""
        new_key = st.text_input("API Key ì…ë ¥", value=current_key, type="password", key="admin_api_key")
        
        if st.button("ğŸ’¾ API Key ì €ì¥", type="primary"):
            if new_key:
                with open(".env", "w") as f:
                    f.write(f"GOOGLE_API_KEY={new_key}")
                st.success("API Keyê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤! (ìƒˆë¡œê³ ì¹¨ í›„ ì ìš©)")
                load_dotenv(override=True)
                st.rerun()
        
        st.markdown("---")
        st.subheader("ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬")
        
        if api_key:
            vectorstore = get_vectorstore(api_key)
            if vectorstore:
                total_count = vectorstore._collection.count()
                col1, col2 = st.columns(2)
                col1.metric("ì´ í•™ìŠµ ì²­í¬", f"{total_count}ê°œ")
                col2.success("DB ìƒíƒœ: ì •ìƒ (antigravity_docs)")
                
                if st.button("ğŸ”„ ì „ì²´ DB ì¬êµ¬ì¶•/ê°±ì‹  (ê¸°ì¡´ ë°ì´í„° ì‚­ì œë¨)"):
                    with st.spinner("ê¸°ì¡´ DB ì‚­ì œ ë° ì¬í•™ìŠµ ì¤‘..."):
                        get_vectorstore.clear()
                        import gc
                        gc.collect()
                        build_vectorstore(api_key)
                        st.success("DBê°€ ì„±ê³µì ìœ¼ë¡œ ì¬êµ¬ì¶•ë˜ì—ˆìŠµë‹ˆë‹¤!")
                        st.rerun()
                
                with st.expander("ğŸ” ë°ì´í„° ìƒ˜í”Œë§"):
                    docs = vectorstore.get(limit=3)
                    st.json(docs)
            else:
                st.warning("DBê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  'DB ìƒì„±'ì„ ì§„í–‰í•˜ì„¸ìš”.")
                if st.button("ğŸ†• DB ìƒì„± ì‹œì‘"):
                     with st.spinner("DB ìƒì„± ì¤‘..."):
                        build_vectorstore(api_key)
                        st.success("ì™„ë£Œ!")
                        st.rerun()
        else:
            st.error("API Keyê°€ ë¨¼ì € ì„¤ì •ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")

    with tab2:
        st.subheader("í•™ìŠµ ë¬¸ì„œ ì—…ë¡œë“œ")
        uploaded_files = st.file_uploader("PDF ë˜ëŠ” TXT íŒŒì¼ ì—…ë¡œë“œ", type=["pdf", "txt"], accept_multiple_files=True)
        if uploaded_files:
            for uploaded_file in uploaded_files:
                file_path = os.path.join(data_dir, uploaded_file.name)
                with open(file_path, "wb") as f:
                    f.write(uploaded_file.getbuffer())
            st.success(f"{len(uploaded_files)}ê°œ íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ! 'DB ì„¤ì •' íƒ­ì—ì„œ ê°±ì‹  ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
        
        st.markdown("---")
        st.subheader("í˜„ì¬ ì €ì¥ëœ íŒŒì¼ ëª©ë¡")
        if os.path.exists(data_dir):
            files = os.listdir(data_dir)
            if files:
                st.dataframe({"íŒŒì¼ëª…": files}, use_container_width=True)
            else:
                st.info("ì €ì¥ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

# --- Main Execution ---

# Load Env
load_dotenv()
env_api_key = os.getenv("GOOGLE_API_KEY", "")

# Sidebar Navigation
with st.sidebar:
    st.header("ğŸ¤– ë©”ë‰´")
    page = st.radio("ì´ë™", ["ğŸ’¬ ì±„íŒ…í•˜ê¸°", "ğŸ› ï¸ ê´€ë¦¬ì ì„¤ì •"], index=0)
    
    st.markdown("---")
    st.caption("Current Info")
    if env_api_key:
        st.success("API Key: í™•ì¸ë¨")
    else:
        st.error("API Key: ì—†ìŒ")

# Path Setup
current_dir = os.path.dirname(os.path.abspath(__file__))
data_dir = os.path.join(current_dir, "data")
if not os.path.exists(data_dir):
    os.makedirs(data_dir)

# Routing
if page == "ğŸ’¬ ì±„íŒ…í•˜ê¸°":
    # Need to load vectorstore for chat
    vectorstore = get_vectorstore(env_api_key) if env_api_key else None
    page_chat(env_api_key, vectorstore)
elif page == "ğŸ› ï¸ ê´€ë¦¬ì ì„¤ì •":
    page_admin(env_api_key, current_dir, data_dir)
