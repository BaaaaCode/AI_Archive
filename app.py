import streamlit as st
import os
from dotenv import load_dotenv

# Fix for UnicodeEncodeError on Windows with Gemini/gRPC
os.environ["PYTHONIOENCODING"] = "utf-8"
os.environ["LANG"] = "C.UTF-8"

# Load environment variables
load_dotenv()

from kiwipiepy import Kiwi
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from pypdf import PdfReader

@st.cache_resource
def get_kiwi():
    return Kiwi()

def tokenize_kiwi(text):
    """
    Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸°ë¡œ í…ìŠ¤íŠ¸ë¥¼ ê³µë°±ìœ¼ë¡œ ì—°ê²°ëœ ë¬¸ìì—´ë¡œ ë³€í™˜
    """
    kiwi = get_kiwi()
    results = kiwi.analyze(text)
    tokens = []
    for result in results:
        for token in result[0]:
            tokens.append(token.form)
    return ' '.join(tokens)

def preprocess_and_chunk(text):
    """
    1. Kiwi í˜•íƒœì†Œ ë¶„ì„
    2. RecursiveCharacterTextSplitterë¡œ ì²­í‚¹ (size=300, overlap=50)
    """
    processed_text = tokenize_kiwi(text)

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=300,
        chunk_overlap=50,
        separators=["\n\n", "\n", " ", ""]
    )
    return text_splitter.split_text(processed_text)

@st.cache_resource
def get_vectorstore(api_key):
    """
    Load existing ChromaDB if available.
    """
    persist_dir = "./antigravity_db"
    
    if os.path.exists(persist_dir) and os.listdir(persist_dir):
        embeddings = GoogleGenerativeAIEmbeddings(
            model="models/embedding-001", 
            google_api_key=api_key
        )
        vectorstore = Chroma(
            persist_directory=persist_dir,
            embedding_function=embeddings,
            collection_name="antigravity_docs"
        )
        return vectorstore
    return None

import chromadb

def build_vectorstore(api_key):
    """
    Build new ChromaDB from data folder.
    Refreshes the DB by deleting the collection via client (safer on Windows).
    """
    persist_dir = "./antigravity_db"
    current_dir = os.path.dirname(os.path.abspath(__file__))
    data_dir = os.path.join(current_dir, "data")
    
    if not os.path.exists(data_dir):
        st.error("Data directory not found.")
        return None

    # Gather all texts
    all_chunks = []
    files = os.listdir(data_dir)
    status_text = st.empty()
    
    progress_bar = st.progress(0)
    for i, file in enumerate(files):
        status_text.text(f"Processing {file}...")
        file_path = os.path.join(data_dir, file)
        
        content = ""
        try:
            if file.lower().endswith(".pdf"):
                pdf = PdfReader(file_path)
                for page in pdf.pages:
                    text = page.extract_text()
                    if text:
                        content += text + "\n"
            else: # Default to text file
                with open(file_path, "r", encoding="utf-8") as f:
                    content = f.read()
            
            if content:
                chunks = preprocess_and_chunk(content)
                all_chunks.extend(chunks)
                
        except Exception as e:
            st.error(f"Error reading {file}: {e}")
            continue
            
        progress_bar.progress((i + 1) / len(files))
    
    status_text.text(f"Generating Embeddings for {len(all_chunks)} chunks...")
    
    embeddings = GoogleGenerativeAIEmbeddings(
        model="models/embedding-001", 
        google_api_key=api_key
    )
    
    # Use PersistentClient to handle collection reset
    client = chromadb.PersistentClient(path=persist_dir)
    
    # Try to delete existing collection to avoid duplicates
    try:
        client.delete_collection("antigravity_docs")
    except ValueError:
        pass # Collection might not exist yet
    
    vectorstore = Chroma.from_texts(
        texts=all_chunks,
        embedding=embeddings,
        collection_name="antigravity_docs",
        client=client # Pass client directly
    )
    status_text.success("DB successfully built!")
    return vectorstore

def ask_gemini(vectorstore, question, api_key, chat_history):
    # 1. Morphological Analysis of the Question (Using Kiwi as replacement for Okt)
    processed_question = tokenize_kiwi(question)
    
    # 2. Retrieve Top 5 Documents (Increased from 3 to improve recall)
    # Return (doc, score) tuples
    docs_with_scores = vectorstore.similarity_search_with_score(processed_question, k=5)
    
    # Extract docs just for context building
    docs = [doc for doc, score in docs_with_scores]
    context = "\n\n".join([doc.page_content for doc in docs])
    
    # Format chat history for context
    # Use last 3 turns to keep prompt size manageable
    recent_history = chat_history[-6:] if len(chat_history) > 6 else chat_history
    formatted_history = ""
    for msg in recent_history:
        role = "User" if msg["role"] == "user" else "Assistant"
        formatted_history += f"{role}: {msg['content']}\n"

    # 3. System Prompt & Generation
    system_prompt = f"""
    ë„ˆëŠ” ë²•ë¥  ì „ë¬¸ê°€ì•¼. ì•„ë˜ì˜ [Context]ì™€ [Chat History]ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ ë‹µë³€í•´ì¤˜.
    ë§Œì•½ [Context]ì— ì—†ëŠ” ë‚´ìš©ì´ë¼ë©´ "ì œê³µëœ ë¬¸ì„œì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µí•´ì¤˜.
    ì˜¤ì§ ì œê³µëœ ë§¥ë½ ì •ë³´ë§Œ ì°¸ê³ í•´ì„œ ë‹µë³€í•´ì•¼ í•´.
    
    [Context]
    {context}
    
    [Chat History]
    {formatted_history}
    
    [Question]
    {question}
    """
    
    # Using gemini-2.5-flash-lite as suggested by user
    llm = ChatGoogleGenerativeAI(
        model="models/gemini-2.5-flash-lite",
        google_api_key=api_key,
        temperature=0
    )
    
    # Create a generator for streaming
    def stream_func():
        for chunk in llm.stream(system_prompt):
            yield chunk.content

    return stream_func(), docs

def summarize_references(docs, api_key):
    """
    References are in tokenized format (e.g. 'ì œ 4 ì¡° ...').
    Use AI to reconstruct natural Korean and summarize.
    """
    content = "\n\n".join([doc.page_content for doc in docs])
    
    system_prompt = f"""
    ì•„ë˜ í…ìŠ¤íŠ¸ëŠ” í˜•íƒœì†Œ ë¶„ì„ê¸°ì— ì˜í•´ í† í°í™”ë˜ì–´ ë„ì–´ì“°ê¸°ê°€ ì–´ìƒ‰í•œ í•œêµ­ì–´ ë¬¸ì„œë“¤ì…ë‹ˆë‹¤.
    ì´ ë‚´ìš©ì„ ì½ê³ , ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ë¬¸ì¥ìœ¼ë¡œ ë‹¤ë“¬ì–´ì„œ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.
    ë²•ë¥  ì „ë¬¸ê°€ì²˜ëŸ¼ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì •ë¦¬í•´ì£¼ì„¸ìš”.
    
    [Raw Refereces]
    {content}
    """
    
    llm = ChatGoogleGenerativeAI(
        model="models/gemini-2.5-flash-lite",
        google_api_key=api_key,
        temperature=0
    )
    
    response = llm.invoke(system_prompt)
    return response.content

# 1. Page Config
st.set_page_config(
    page_title="ì‹¤í—˜ìš© ì±—ë´‡",
    page_icon="ğŸ¤–",
    layout="wide"
)

# 2. Main Title
st.title("ğŸš€ RAG Chatbot")

# 3. Sidebar
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    
    # Define data directory first to avoid NameError
    current_dir = os.path.dirname(os.path.abspath(__file__))
    data_dir = os.path.join(current_dir, "data")
    if not os.path.exists(data_dir):
        os.makedirs(data_dir)
    
    # Load API Key from env if available
    env_api_key = os.getenv("GOOGLE_API_KEY", "")
    api_key = st.text_input("Google API Key", value=env_api_key, type="password")
    
    if st.button("ğŸ’¾ API Key ì €ì¥ (ë¡œì»¬ .env)"):
        if api_key:
            with open(".env", "w") as f:
                f.write(f"GOOGLE_API_KEY={api_key}")
            st.success("API Keyê°€ .env íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
            # Reload to apply immediately
            load_dotenv(override=True)
            st.rerun()
        else:
            st.warning("API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    
    # File Upload Section
    st.markdown("---")
    st.header("ğŸ“‚ ë°ì´í„° ì—…ë¡œë“œ")
    uploaded_file = st.file_uploader("í•™ìŠµì‹œí‚¬ íŒŒì¼ì„ ì˜¬ë ¤ì£¼ì„¸ìš” (.txt, .pdf)", type=["txt", "pdf"])
    if uploaded_file:
        file_path = os.path.join(data_dir, uploaded_file.name)
        with open(file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        st.success(f"'{uploaded_file.name}' ì €ì¥ ì™„ë£Œ! ì•„ë˜ [DB êµ¬ì¶•í•˜ê¸°]ë¥¼ ëˆŒëŸ¬ ë°˜ì˜í•´ì£¼ì„¸ìš”.")
    
    st.markdown("---")
    st.header("ğŸ—„ï¸ ë°ì´í„° ë² ì´ìŠ¤ ìƒíƒœ")
    
    if api_key:
        vectorstore = get_vectorstore(api_key)
        
        if vectorstore:
            st.success("âœ… DB ë¡œë“œ ì™„ë£Œ (antigravity_docs)")
            
            # DB Inspection Feature
            with st.expander("ğŸ” DB ë‚´ë¶€ ë°ì´í„° í™•ì¸"):
                try:
                    collection_data = vectorstore.get(limit=3) 
                    
                    if collection_data and 'documents' in collection_data:
                        docs = collection_data['documents']
                        ids = collection_data['ids']
                        
                        total_count = vectorstore._collection.count()
                        st.write(f"ğŸ“Š **ì´ ì²­í¬ ìˆ˜:** {total_count}ê°œ")
                        
                        st.write("ğŸ§© **ìƒ˜í”Œ ë°ì´í„° (ìµœëŒ€ 3ê°œ):**")
                        for i, doc in enumerate(docs):
                            st.caption(f"**Chunk {ids[i]}:**")
                            st.text(doc[:100] + "...") 
                    else:
                        st.write("ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                except Exception as e:
                    st.error(f"ë°ì´í„° í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # Rebuild DB button for updating data
            if st.button("ğŸ”„ DB ê°±ì‹ í•˜ê¸°"):
                 with st.spinner("ë°ì´í„° ì²˜ë¦¬ ì¤‘..."):
                      # 1. Release existing resources
                      get_vectorstore.clear()
                      if 'vectorstore' in locals():
                          del vectorstore
                      import gc
                      gc.collect()
                      
                      # 2. Build new DB
                      vectorstore = build_vectorstore(api_key)
                      st.rerun()

        else:
            st.warning("âš ï¸ DBê°€ ì—†ìŠµë‹ˆë‹¤.")
            if st.button("DB êµ¬ì¶•í•˜ê¸°"):
                 with st.spinner("ë°ì´í„° ì²˜ë¦¬ ì¤‘..."):
                      # Release resources just in case
                      get_vectorstore.clear() 
                      import gc
                      gc.collect()
                      
                      vectorstore = build_vectorstore(api_key)
                      st.rerun()
    else:
        st.info("API Keyë¥¼ ì…ë ¥í•˜ë©´ DB ìƒíƒœë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")


        files = os.listdir(data_dir)
        if files:
            st.markdown("---")
            st.write(f"ğŸ“‚ **ì†ŒìŠ¤ íŒŒì¼ ({len(files)}ê°œ):**")
            for f in files:
                st.caption(f"- {f}")
        else:
            st.warning("âš ï¸ data í´ë”ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")

# 4. Chat Interface
if "messages" not in st.session_state:
    st.session_state.messages = []

# Display chat messages from history on app rerun
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# React to user input
if prompt := st.chat_input("ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”..."):
    # Display user message
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # Display assistant response
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        
        if not api_key:
            response = "âš ï¸ ì‚¬ì´ë“œë°”ì—ì„œ Google API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."
            message_placeholder.warning(response)
        elif not vectorstore:
             response = "âš ï¸ DBê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. DBë¥¼ ë¨¼ì € êµ¬ì¶•í•´ì£¼ì„¸ìš”."
             message_placeholder.warning(response)
        else:
            # Change spinner context to allow streaming write
            with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                try:
                    stream, docs = ask_gemini(vectorstore, prompt, api_key, st.session_state.messages)
                    
                    # Use st.write_stream to simulate typing effect
                    # write_stream returns the full concatenated string
                    response_text = message_placeholder.write_stream(stream)
                    
                    # Optional: Show sources in expander using AI summary
                    with st.expander("ğŸ“š ì°¸ì¡° ë¬¸ì„œ (AI ìš”ì•½)"):
                         # Check if response indicates failure to find info
                         # Only skip if the response is short (pure refusal)
                         # If it's a long partial answer (e.g. "Definition not found, but types are..."), show summary.
                         if "ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in response_text and len(response_text) < 150:
                             st.info("ğŸ’¡ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ìš”ì•½ì„ ìƒëµí•©ë‹ˆë‹¤. ì›ë¬¸ ë°ì´í„°ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
                             for i, doc in enumerate(docs):
                                st.caption(f"**Ref {i+1}**")
                                st.text(doc.page_content)
                         else:
                             with st.spinner("ì°¸ì¡° ë¬¸ì„œ ìš”ì•½ ì¤‘..."):
                                summary = summarize_references(docs, api_key)
                                st.markdown(summary)
                                
                                st.caption("---")
                                st.caption("ğŸ” ì›ë¬¸ ë°ì´í„° (í† í°í™”ë¨)")
                                for i, doc in enumerate(docs):
                                    st.text(f"[Ref {i+1}] {doc.page_content[:100]}...")
                            
                    response = response_text # For history
                except Exception as e:
                    response = f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}"
                    message_placeholder.error(response)

    st.session_state.messages.append({"role": "assistant", "content": response})
